{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9772821,"sourceType":"datasetVersion","datasetId":5986231}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.optimizers import Adam\n\n\n# Define the dataset path\ndata_dir = '/kaggle/input/tomatodataset/Tomato Dataset (final)'\n\n# Load datasets with one-hot encoding of labels\ntrain_dataset = image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=123,\n    image_size=(227, 227),\n    batch_size=32,\n    label_mode='categorical'  # Ensures one-hot encoding\n)\n\nval_dataset = image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=123,\n    image_size=(227, 227),\n    batch_size=32,\n    label_mode='categorical'  # Ensures one-hot encoding\n)\n\n# Define the base model\nbase_model = EfficientNetB0(input_shape=(227, 227, 3), include_top=False, weights='imagenet')\nbase_model.trainable = False  # Freeze the base model initially\n\n# Add custom classification layers with Batch Normalization and L2 regularization\nmodel = models.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n    layers.BatchNormalization(),  # Batch Normalization layer\n    layers.Activation('relu'),\n    layers.Dropout(0.5),\n    layers.Dense(3, activation='softmax')  # Assuming 5 classes\n])\n\n# Compile the model using categorical crossentropy\nmodel.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Callbacks\ncheckpoint_cb = ModelCheckpoint('best_model.keras', save_best_only=True)\nearly_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n# Learning rate scheduler\ndef lr_schedule(epoch):\n    return 1e-3 * 10 ** (epoch / 20)\n\nlr_scheduler = LearningRateScheduler(lr_schedule)\n\n# Train the model initially with a learning rate scheduler\nhistory = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=20,\n    callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler]\n)\n\n# Unfreeze some layers of the base model for fine-tuning\nbase_model.trainable = True\nfor layer in base_model.layers[:-30]:  # Freeze all layers except the last 30\n    layer.trainable = False\n\n# Recompile the model for fine-tuning with a lower learning rate\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model with fine-tuning and callbacks\nfine_tune_history = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=20,\n    callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler]\n)\n\n# Save the final model\n#model.save('efficientnet_soybean_final_model.keras')\n\n# Load the test dataset for final evaluation\ntest_dataset = image_dataset_from_directory(\n    data_dir,  # Same directory, or separate if you have a distinct test set\n    validation_split=0.2,  # If using a separate test set, remove this line\n    subset=\"validation\",  # Use this if your test set is split from the original dataset\n    seed=123,\n    image_size=(227, 227),\n    batch_size=32,\n    label_mode='categorical'  # Ensure one-hot encoding is consistent\n)\n\n# Evaluate the model on the test dataset\ntest_loss, test_accuracy = model.evaluate(test_dataset)\n\nprint(f'Test Accuracy: {test_accuracy:.4f}')\n\nmodel.save('tomato.keras')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T16:23:52.541626Z","iopub.execute_input":"2024-10-31T16:23:52.542258Z","iopub.status.idle":"2024-10-31T16:29:00.350530Z","shell.execute_reply.started":"2024-10-31T16:23:52.542218Z","shell.execute_reply":"2024-10-31T16:29:00.349298Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 9000 files belonging to 3 classes.\nUsing 7200 files for training.\nFound 9000 files belonging to 3 classes.\nUsing 1800 files for validation.\nEpoch 1/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.8125 - loss: 0.8895 - val_accuracy: 0.9017 - val_loss: 0.5372 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.8920 - loss: 0.5374 - val_accuracy: 0.8950 - val_loss: 0.4695 - learning_rate: 0.0011\nEpoch 3/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.9041 - loss: 0.4454 - val_accuracy: 0.8789 - val_loss: 0.4261 - learning_rate: 0.0013\nEpoch 4/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - accuracy: 0.9059 - loss: 0.3944 - val_accuracy: 0.8283 - val_loss: 0.5412 - learning_rate: 0.0014\nEpoch 5/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - accuracy: 0.9014 - loss: 0.3908 - val_accuracy: 0.8106 - val_loss: 0.6450 - learning_rate: 0.0016\nEpoch 6/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.8936 - loss: 0.3918 - val_accuracy: 0.8900 - val_loss: 0.3872 - learning_rate: 0.0018\nEpoch 7/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.9080 - loss: 0.3747 - val_accuracy: 0.8883 - val_loss: 0.3947 - learning_rate: 0.0020\nEpoch 8/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.8883 - loss: 0.4176 - val_accuracy: 0.9117 - val_loss: 0.3686 - learning_rate: 0.0022\nEpoch 9/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.9027 - loss: 0.3957 - val_accuracy: 0.8817 - val_loss: 0.4554 - learning_rate: 0.0025\nEpoch 10/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8994 - loss: 0.4294 - val_accuracy: 0.8617 - val_loss: 0.5124 - learning_rate: 0.0028\nEpoch 11/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - accuracy: 0.8884 - loss: 0.4506 - val_accuracy: 0.8939 - val_loss: 0.4340 - learning_rate: 0.0032\nEpoch 1/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 80ms/step - accuracy: 0.8773 - loss: 0.5031 - val_accuracy: 0.9311 - val_loss: 0.2635 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.9204 - loss: 0.2960 - val_accuracy: 0.9067 - val_loss: 0.2590 - learning_rate: 0.0011\nEpoch 3/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.9319 - loss: 0.2393 - val_accuracy: 0.9400 - val_loss: 0.2074 - learning_rate: 0.0013\nEpoch 4/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.9389 - loss: 0.2149 - val_accuracy: 0.9283 - val_loss: 0.2331 - learning_rate: 0.0014\nEpoch 5/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.9390 - loss: 0.1960 - val_accuracy: 0.9344 - val_loss: 0.2717 - learning_rate: 0.0016\nEpoch 6/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.9386 - loss: 0.2027 - val_accuracy: 0.9450 - val_loss: 0.2056 - learning_rate: 0.0018\nEpoch 7/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.9490 - loss: 0.1836 - val_accuracy: 0.9522 - val_loss: 0.1654 - learning_rate: 0.0020\nEpoch 8/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.9513 - loss: 0.1694 - val_accuracy: 0.9356 - val_loss: 0.2151 - learning_rate: 0.0022\nEpoch 9/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.9530 - loss: 0.1630 - val_accuracy: 0.9400 - val_loss: 0.2448 - learning_rate: 0.0025\nEpoch 10/20\n\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.9523 - loss: 0.1768 - val_accuracy: 0.9556 - val_loss: 0.2081 - learning_rate: 0.0028\nFound 9000 files belonging to 3 classes.\nUsing 1800 files for validation.\n\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9564 - loss: 0.1472\nTest Accuracy: 0.9522\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report\n\n# Get the true labels and predicted labels from the validation set\ntrue_labels = []\npredicted_labels = []\n\nfor images, labels in val_dataset:\n    # Predict the labels for each batch of images\n    predictions = model.predict(images)\n    predicted_labels.extend(np.argmax(predictions, axis=1))\n    true_labels.extend(np.argmax(labels.numpy(), axis=1))\n\n# Generate the classification report\nclass_names = val_dataset.class_names  # Get the class names from the dataset\nreport = classification_report(true_labels, predicted_labels, target_names=class_names)\n\n# Print the classification report\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:19:34.463084Z","iopub.execute_input":"2024-10-31T14:19:34.464418Z","iopub.status.idle":"2024-10-31T14:19:50.226779Z","shell.execute_reply.started":"2024-10-31T14:19:34.464349Z","shell.execute_reply":"2024-10-31T14:19:50.225790Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n              precision    recall  f1-score   support\n\n        HIGH       0.94      0.93      0.94       623\n         LOW       0.99      0.98      0.99       591\n      MEDIUM       0.92      0.94      0.93       586\n\n    accuracy                           0.95      1800\n   macro avg       0.95      0.95      0.95      1800\nweighted avg       0.95      0.95      0.95      1800\n\n","output_type":"stream"}]}]}